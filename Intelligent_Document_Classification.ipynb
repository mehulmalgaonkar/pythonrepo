{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intelligent_Document_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehulmalgaonkar/pythonrepo/blob/main/Intelligent_Document_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Eq14J7kDjtt"
      },
      "source": [
        "<B>Intelligent Document Classification</B>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YNsn6iuDf2j",
        "outputId": "40006db1-c161-422e-f6ea-08c14681d7f6"
      },
      "source": [
        "print(\"Let's Start\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's Start\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf4DsN7RB7qd",
        "outputId": "60c083ba-424e-4a35-9723-fe4fad1028dc"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 29.2 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20 kB 34.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30 kB 41.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 4.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73052 sha256=f7166087c56b95d4838208575d796a5634cb3f3e8b07c14d0563a4d5c65fb6e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NrnYw8Fl_ko"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!touch ~/.kaggle/kaggle.json\n",
        "\n",
        "api_token = {\"username\":\"add_username\",\"key\":\"add_key\"}\n",
        "\n",
        "import json\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSOr26mkg8kW"
      },
      "source": [
        "# ! kaggle datasets download -d nbhativp/first-half-training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mEUcwGlyMiI"
      },
      "source": [
        "# ! kaggle datasets download -d orllem456/rvlcdip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq4ipSdwzNnW",
        "outputId": "4e73aaff-33eb-49b9-8a62-2dde34b10ccf"
      },
      "source": [
        "! kaggle datasets download -d shaz13/real-world-documents-collections"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "401 - Unauthorized\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkVzpQ8pJRnz"
      },
      "source": [
        "/content/real-world-documents-collections.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_acnp7wLoVjq"
      },
      "source": [
        "# importing required modules\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUfypK27oVde"
      },
      "source": [
        "# specifying the zip file name\n",
        "file_name = \"real-world-documents-collections.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "eJBcxNRdobwK",
        "outputId": "399f1cc2-321d-437f-ae8c-3f578e45b7bc"
      },
      "source": [
        "# opening the zip file in READ mode\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "    # printing all the contents of the zip file\n",
        "    # zip.printdir()\n",
        "\n",
        "    # extracting all the files\n",
        "    print('Extracting all the files now...')\n",
        "    zip.extractall()\n",
        "    print('Done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-0a01e57d975e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# opening the zip file in READ mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# printing all the contents of the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# zip.printdir()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'real-world-documents-collections.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J3PhIE1gu3k"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbVumUowooph"
      },
      "source": [
        "folderlist=os.listdir(\"./docs-sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qx8RqA7qnt2"
      },
      "source": [
        "folderlist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oeiQ1gZqkz5"
      },
      "source": [
        "required_folders = ['form','resume','letter','invoice', 'questionnaire']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROixf6nepDDR"
      },
      "source": [
        "parent=\"./docs-sm\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q1OonnpMM_s"
      },
      "source": [
        "/content/docs-sm/questionnaire"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeG97wu6g6TH"
      },
      "source": [
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ckDWB6coVAF"
      },
      "source": [
        "for x in folderlist:\n",
        "  if x not in required_folders:\n",
        "    path = os.path.join(parent, x) \n",
        "    #os.rmdir(path) \n",
        "    shutil.rmtree(path, ignore_errors=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yROf48auway"
      },
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "image = Image.open(\"/content/docs-sm/invoice/0000137486.jpg\")\n",
        "#image = image.convert(\"RGB\")\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBypu6wmuwZG"
      },
      "source": [
        "image = Image.open(\"/content/docs-sm/questionnaire/0000002206.jpg\")\n",
        "image = image.convert(\"RGB\")\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BHrFUXQXum-"
      },
      "source": [
        "! sudo apt install tesseract-ocr\n",
        "! pip install pytesseract"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeDNsyiwXugx"
      },
      "source": [
        "import pytesseract\n",
        "import numpy as np\n",
        "\n",
        "ocr_df = pytesseract.image_to_data(image, output_type='data.frame')\n",
        "ocr_df = ocr_df.dropna().reset_index(drop=True)\n",
        "float_cols = ocr_df.select_dtypes('float').columns\n",
        "ocr_df[float_cols] = ocr_df[float_cols].round(0).astype(int)\n",
        "ocr_df = ocr_df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "words = ' '.join([word for word in ocr_df.text if str(word) != 'nan'])\n",
        "words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6B8BgF-rsmf"
      },
      "source": [
        "coordinates = ocr_df[['left', 'top', 'width', 'height']]\n",
        "actual_boxes = []\n",
        "for idx, row in coordinates.iterrows():\n",
        "    x, y, w, h = tuple(row) # the row comes in (left, top, width, height) format\n",
        "    actual_box = [x, y, x+w, y+h] # we turn it into (left, top, left+width, top+height) to get the actual box \n",
        "    actual_boxes.append(actual_box)\n",
        "\n",
        "draw = ImageDraw.Draw(image, \"RGB\")\n",
        "for box in actual_boxes:\n",
        "  draw.rectangle(box, outline='red')\n",
        "\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhbfoZ1zAk1X"
      },
      "source": [
        "width, height = image.size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU65-VdNAnWd"
      },
      "source": [
        "ocr_df = pytesseract.image_to_data(image, output_type='data.frame')\n",
        "float_cols = ocr_df.select_dtypes('float').columns\n",
        "ocr_df = ocr_df.dropna().reset_index(drop=True)\n",
        "ocr_df[float_cols] = ocr_df[float_cols].round(0).astype(int)\n",
        "ocr_df = ocr_df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "ocr_df = ocr_df.dropna().reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcaDiK-mA57K"
      },
      "source": [
        "ocr_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn3ELSJ-BIS4"
      },
      "source": [
        "words = list(ocr_df.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foc3A_I3BJy6"
      },
      "source": [
        "words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D9LBkBkJDPx"
      },
      "source": [
        "import cv2 as cv\n",
        "import math\n",
        "from scipy import ndimage\n",
        "import pytesseract as pt\n",
        "import pandas as pd\n",
        "import os "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t36WN_wIIw3l"
      },
      "source": [
        "count=0\n",
        "df = pd.DataFrame(columns=['text', 'doc_type'])\n",
        "kernel= np.ones((1,1),np.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS0qe0qRU3gA"
      },
      "source": [
        "os.path.basename(os.path.dirname(\"/content/docs-sm/form/0000980962.jpg\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw0_OrR_Vfdk"
      },
      "source": [
        "os.path.basename(os.path.dirname(\"/content/docs-sm/form\\\\0000980962.jpg\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbBD0jYSTKY5"
      },
      "source": [
        "for subdir, dirs, files in os.walk(\"docs-sm\"):\n",
        "  print(subdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "AtNIJEk5Iw0u",
        "outputId": "2878c236-d408-453e-b8b2-2d70bae4feff"
      },
      "source": [
        "for subdir, dirs, files in os.walk(\"docs-sm\"):\n",
        "    for file in files:\n",
        "        im = Image.open(subdir+\"/\"+file)\n",
        "        img=np.asarray(im)\n",
        "        # #to convert into gray scale\n",
        "        # img=cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
        "        #for tilted images\n",
        "        img_edges = cv.Canny(img, 100, 100, apertureSize=3)\n",
        "        lines = cv.HoughLinesP(img_edges, 1, math.pi / 180.0, 100, minLineLength=100, maxLineGap=5)\n",
        "        angles = []\n",
        "        if lines is not None:\n",
        "            for x1, y1, x2, y2 in lines[0]:\n",
        "                cv.line(img, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
        "                angle = math.degrees(math.atan2(y2 - y1, x2 - x1))\n",
        "                angles.append(angle)\n",
        "            median_angle = np.median(angles)\n",
        "            if (median_angle != 0) :\n",
        "                img = ndimage.rotate(img, median_angle)\n",
        "        #to read text using tesseract\n",
        "        result=pt.image_to_data(img, output_type='data.frame')\n",
        "        #to remove words with less confidence\n",
        "        result = result[result.conf > 75]\n",
        "        result_mean = result[\"conf\"].mean()\n",
        "        #to apply image preprocessing\n",
        "        ret,thresh1 = cv.threshold(img,110,255,cv.THRESH_BINARY)\n",
        "        blur = cv.bilateralFilter(thresh1,9,100,100)\n",
        "        result_after_preproc = pt.image_to_data(blur, output_type='data.frame')\n",
        "        result_after_preproc = result_after_preproc[result_after_preproc.conf > 75]\n",
        "        result_after_preproc_mean= result_after_preproc[\"conf\"].mean()\n",
        "        if (result_mean<result_after_preproc_mean):\n",
        "            result=result_after_preproc\n",
        "        #to keep track of progress\n",
        "        count=count+1\n",
        "        print(count)\n",
        "        #to extract file name and set it as class\n",
        "        path=os.path.dirname(subdir+\"/\"+file)\n",
        "        #joining words to form sentences\n",
        "        text=''\n",
        "        for a in result['text']:\n",
        "            text=text+' '+ str(a)\n",
        "        df = df.append({'text': text, 'doc_type': os.path.basename(path)}, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-949a0e6d5975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"docs-sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# #to convert into gray scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poncH2PULcXP"
      },
      "source": [
        " df.to_csv ('df.csv', encoding='utf-8', index = None, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vdmh-PMIwxt"
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtukWkIVoI8n"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voN2-UgyhMeW"
      },
      "source": [
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_-dbpq9jOEJ"
      },
      "source": [
        "shutil.copy(\"df.csv\", \"Add_Path/df.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuhdxHOlGPKq"
      },
      "source": [
        "Load model for nlp using spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur56AQ520S_x"
      },
      "source": [
        "! python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UfbXixrQk6M"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy \n",
        "from  spacy.lang.en.stop_words import STOP_WORDS\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import en_core_web_sm\n",
        "import string\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS7MJUaNGPKr"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDC3nXIsGPKr"
      },
      "source": [
        "stopwords = list(STOP_WORDS)\n",
        "print(stopwords[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R3NBANvGPKr"
      },
      "source": [
        "# Remove / and - from punctuations for dates\n",
        "punctuations = string.punctuation\n",
        "print(punctuations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCpiB25LGPKr"
      },
      "source": [
        "Function to remove stopwords, punctuations and tokanize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br1rPuhGGPKs"
      },
      "source": [
        "# Function to tokenise the text\n",
        "def tokeniser(sentence):\n",
        "    \n",
        "    # Process the text\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Rule 1\n",
        "    # Convert tokens to lemma form for all except '-PRON-'\n",
        "    # Remember (from SpaCy introduction) : Tokens like I, my, me were represented as '-PRON-' by lemma attribute\n",
        "    tokens = [ token.lemma_.lower().strip() if token.lemma_ != \"-PRON-\" else token.lower_ for token in doc ]\n",
        "\n",
        "    # Rule 2\n",
        "    # Remove stop words and punctuation\n",
        "    tokens = [ token for token in tokens if token not in stopwords and token not in punctuations ]\n",
        "    \n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EaphsDQGPKs"
      },
      "source": [
        "Vectorisation using sklearn tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYeUJRkYGPKs"
      },
      "source": [
        "tfvectorizer = TfidfVectorizer(tokenizer = tokeniser)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1l59bb5GPKs"
      },
      "source": [
        "Train-test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3VuzlnhGPKt"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD_MWoT0GPKt"
      },
      "source": [
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIzYiA4jGPKt"
      },
      "source": [
        "df.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSnBm4TKGPKt"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGBWtd0iRCf9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weyTDGLeGPKu"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split( df['text'], df['doc_type'], \n",
        "                                                    test_size = 0.2, random_state = 678)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-mVg0-JGPKu"
      },
      "source": [
        "Create machine learning pipe-line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5sOGDCnGPKu"
      },
      "source": [
        "<b>Naive Bayes & TF-IDF<b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6V8ak6xGPKu"
      },
      "source": [
        "Build a Classifier Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUWv1stMRM8Y"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CebfcmDAGPKu"
      },
      "source": [
        "classifier_NB = MultinomialNB()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JuDyITWRSWy"
      },
      "source": [
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_7487heGPKu"
      },
      "source": [
        "# Create the  pipeline to clean, tokenize, vectorize, and classify using\"Count Vectorizor\"\n",
        "# Multiple models can be added to the Pipeline object to be executed in sequence.\n",
        "model_pipe_NB = Pipeline( [ ('vectorizer', tfvectorizer), \n",
        "                         ('classifier', classifier_NB) ] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9tZV4FuGPKv"
      },
      "source": [
        "Fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-muhud0XGPKv"
      },
      "source": [
        "model_pipe_NB.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uajR_0dEGPKv"
      },
      "source": [
        "Export model for future use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGMscWDlRgSe"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8O0ipHgGPKv"
      },
      "source": [
        "filename = 'NB_model_5cat_data.sav'\n",
        "pickle.dump(model_pipe_NB, open(filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGQMzQ1EGPKv"
      },
      "source": [
        "# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTq-oyxAGPKw"
      },
      "source": [
        "Predict on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU-ZJQ72GPKw"
      },
      "source": [
        "preds = model_pipe_NB.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgiWBsD1GPKw"
      },
      "source": [
        "preds[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNmTb3D5GPKw"
      },
      "source": [
        "X_test[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6qZxKorGPKw"
      },
      "source": [
        "y_test[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho4JJXTCGPKx"
      },
      "source": [
        "Compute accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y32TfPjQGPKx"
      },
      "source": [
        "# Accuracy\n",
        "print(\"Train Accuracy: \", model_pipe_NB.score(X_train, y_train))\n",
        "\n",
        "# Accuracy\n",
        "print(\"Test Accuracy: \", model_pipe_NB.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl1whD6qGPKx"
      },
      "source": [
        "# TF-IDF with diffrent classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc0tSJM1GPKx"
      },
      "source": [
        "https://www.kaggle.com/jeffd23/10-classifier-showdown-in-scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuOHF3jFR8zs"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x41Br5qdGPKy"
      },
      "source": [
        "classifiers = [\n",
        "    KNeighborsClassifier(5),\n",
        "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
        "    DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(),\n",
        "    AdaBoostClassifier(),\n",
        "    GradientBoostingClassifier(),\n",
        "    MultinomialNB()\n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3m1jcQPGPKy"
      },
      "source": [
        "classifier_names = [\n",
        "    'KNeighborsClassifier',\n",
        "    'SVC',\n",
        "    'DecisionTreeClassifier',\n",
        "    'RandomForestClassifier',\n",
        "    'AdaBoostClassifier',\n",
        "    'GradientBoostingClassifier',\n",
        "    'MultinomialNB'\n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCYIMQGKGPKy"
      },
      "source": [
        "# Logging for Visual Comparison\n",
        "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
        "log = pd.DataFrame(columns=log_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS7pQX_rGPKy"
      },
      "source": [
        "Iterate over each classifier and add entry into dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTDP1bZFGPKy"
      },
      "source": [
        "classifiers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDhdLsMOGPKz"
      },
      "source": [
        "count=0\n",
        "classifier_names[count]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNEtP5kgGPKz"
      },
      "source": [
        "count=0\n",
        "for clf in classifiers:\n",
        "    #to create model pipeline for each classifier\n",
        "    model_pipe = Pipeline( [ ('vectorizer', tfvectorizer), \n",
        "                             ('classifier', clf) ] )\n",
        "    #to fit data on model pipe\n",
        "    model_pipe.fit(X_train, y_train)\n",
        "    #to save generated model\n",
        "    filename = classifier_names[count] + '.sav'\n",
        "    pickle.dump(model_pipe, open(filename, 'wb'))\n",
        "    count=count+1\n",
        "    #to print accuracy of each model\n",
        "    name = clf.__class__.__name__\n",
        "    \n",
        "    print(\"=\"*30)\n",
        "    print(name)\n",
        "    \n",
        "    print('****Results****')\n",
        "    test_predictions = model_pipe.predict(X_test)\n",
        "    acc = accuracy_score(y_test, test_predictions)\n",
        "    print(\"Accuracy: {:.4%}\".format(acc))\n",
        "    \n",
        "    test_predictions_prob = model_pipe.predict_proba(X_test)\n",
        "    ll = log_loss(y_test, test_predictions_prob)\n",
        "    print(\"Log Loss: {}\".format(ll))\n",
        "    \n",
        "    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n",
        "    log = log.append(log_entry)\n",
        "    \n",
        "print(\"=\"*30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrIwbGy7W0cr"
      },
      "source": [
        "#Visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI0aONifGPKz"
      },
      "source": [
        "sns.set_color_codes(\"muted\")\n",
        "sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")\n",
        "\n",
        "plt.xlabel('Accuracy %')\n",
        "plt.title('Classifier Accuracy')\n",
        "plt.show()\n",
        "\n",
        "sns.set_color_codes(\"muted\")\n",
        "sns.barplot(x='Log Loss', y='Classifier', data=log, color=\"g\")\n",
        "\n",
        "plt.xlabel('Log Loss')\n",
        "plt.title('Classifier Log Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mLohv-ZGPK0"
      },
      "source": [
        "classifier_RF = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
        "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
        "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                        min_samples_leaf=1, min_samples_split=2,\n",
        "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
        "                        n_jobs=None, oob_score=False, random_state=None,\n",
        "                        verbose=0, warm_start=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnALf9ULGPK0"
      },
      "source": [
        "model_pipe_RF = Pipeline( [ ('vectorizer', tfvectorizer), \n",
        "                         ('classifier', classifier_RF) ] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep_aWki2GPK0"
      },
      "source": [
        "model_pipe_RF.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNUrcrX2GPK1"
      },
      "source": [
        "preds_RF = model_pipe_RF.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFRXiFAbGPK1"
      },
      "source": [
        "preds_RF[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJj3DX9BGPK1"
      },
      "source": [
        "X_test[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiNPLwuPGPK1"
      },
      "source": [
        "y_test[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOLGwGrCGPK2"
      },
      "source": [
        "# Accuracy\n",
        "print(\"Train Accuracy: \", model_pipe_RF.score(X_train, y_train))\n",
        "\n",
        "# Accuracy\n",
        "print(\"Test Accuracy: \", model_pipe_RF.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU1zvdS8aEFJ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lmV_5vGGPK2"
      },
      "source": [
        "confusion_matrix(y_test,preds_RF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1tcaybrGPK3"
      },
      "source": [
        "vectorized_df_X=tfvectorizer.fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvAyrOcqGPK3"
      },
      "source": [
        "vectorized_df_X_test=tfvectorizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuMYEOgWGPK3"
      },
      "source": [
        "classifier_RF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0Myk71tGPK4"
      },
      "source": [
        "grid_param = {\n",
        "    'n_estimators': [50, 75, 100, 120, 200],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'bootstrap': [True, False]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KUwZgh6aei9"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms5Fac1PGPK4"
      },
      "source": [
        "gd_sr = GridSearchCV(estimator=classifier_RF,\n",
        "                     param_grid=grid_param,\n",
        "                     scoring='accuracy',\n",
        "                     cv=3,\n",
        "                     n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDfGfqkrGPK5"
      },
      "source": [
        "gd_sr.fit(vectorized_df_X, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsxG60_4GPK5"
      },
      "source": [
        "best_parameters = gd_sr.best_params_\n",
        "print(best_parameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-CcHZ3AGPK6"
      },
      "source": [
        "gd_sr.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eNQC-WjGPK6"
      },
      "source": [
        "best_result = gd_sr.best_score_\n",
        "print(best_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzHni-ThGPK7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zH_4aN2GPK7"
      },
      "source": [
        "# Accuracy\n",
        "print(\"Train Accuracy: \", gd_sr.score(vectorized_df_X, y_train))\n",
        "\n",
        "# Accuracy\n",
        "print(\"Test Accuracy: \", gd_sr.score(vectorized_df_X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbop5N6J-0PE"
      },
      "source": [
        "###################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJrfhSqz-M_o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}